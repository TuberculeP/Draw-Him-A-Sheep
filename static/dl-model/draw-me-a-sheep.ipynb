{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: graphviz in ./.venv/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: onnx in ./.venv/lib/python3.11/site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxscript in ./.venv/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: onnxruntime in ./.venv/lib/python3.11/site-packages (1.20.1)\n",
      "Requirement already satisfied: ndjson in ./.venv/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: tensorboard in ./.venv/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.55.7)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in ./.venv/lib/python3.11/site-packages (from onnx) (5.29.3)\n",
      "Requirement already satisfied: ml_dtypes in ./.venv/lib/python3.11/site-packages (from onnxscript) (0.5.1)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime) (25.1.24)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.venv/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.venv/lib/python3.11/site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.venv/lib/python3.11/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in ./.venv/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch torchvision torchaudio matplotlib pandas graphviz onnx onnxscript onnxruntime ndjson scikit-learn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndjson\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'sheep', 'countrycode': 'SE', 'timestamp': '2017-03-28 07:09:01.69625 UTC', 'recognized': True, 'key_id': '4609015428415488', 'drawing': [[[59, 59, 52, 50, 44], [203, 192, 170, 135, 119]], [[152, 159, 161, 161, 148], [207, 207, 204, 189, 132]], [[138, 140, 136, 113, 127, 101, 78, 76, 85, 82, 62, 37, 27, 27, 15, 7, 1, 1, 5, 17, 29, 39, 36, 31, 30, 33, 50, 69, 87, 104, 113, 127, 134, 136, 143, 160, 165, 177, 179], [133, 137, 139, 139, 133, 143, 145, 134, 114, 111, 122, 126, 109, 69, 70, 63, 49, 32, 26, 23, 24, 32, 31, 24, 11, 7, 1, 0, 5, 25, 23, 11, 9, 24, 36, 23, 44, 56, 79]], [[133, 149, 182, 189, 188, 214, 215, 211], [150, 138, 132, 128, 117, 115, 95, 88]], [[184, 224, 242, 253, 254], [85, 70, 58, 36, 12]], [[246, 246, 255], [16, 3, 14]]]}\n"
     ]
    }
   ],
   "source": [
    "# Regardons à quoi ressemble le dataset\n",
    "\n",
    "with open(\"datasets/sheep.ndjson\") as f:\n",
    "    reader = ndjson.reader(f)\n",
    "    print(reader.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# à la différence de MNIST où on avait un tuple (tensor, result), ici on devra convertir nous mêmes ces valeurs avec :\n",
    "# result -> word ; tensor -> drawing\n",
    "\n",
    "# On va utiliser le dataset Quick Draw de google, préparons une classe pour charger les fichiers ndjson\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, file, transform):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        self.trueish = 0\n",
    "        self.falseish = 0\n",
    "        \n",
    "        with open(file, \"r\") as f:\n",
    "            reader = ndjson.reader(f)\n",
    "            for item in reader:\n",
    "                image = self._convert_to_image(item[\"drawing\"])\n",
    "                recognized = item['recognized']\n",
    "                if recognized:\n",
    "                    # if self.trueish > 8000:\n",
    "                    #     continue\n",
    "                    self.trueish +=1\n",
    "                else:\n",
    "                    self.falseish +=1\n",
    "                self.data.append((image, recognized))\n",
    "\n",
    "        print('Done !')\n",
    "\n",
    "    def _convert_to_image(self, drawing):\n",
    "        canvas = np.zeros((28, 28), dtype=np.float32)\n",
    "        for stroke in drawing:\n",
    "            x_coords, y_coords = stroke\n",
    "            for x, y in zip(x_coords, y_coords):\n",
    "                x = int((x / 255) * 27)\n",
    "                y = int((y / 255) * 27)\n",
    "                canvas[y, x] = 1\n",
    "        return canvas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # on applique notre transform à la demande\n",
    "    def __getitem__(self, idx):\n",
    "        data, recognized = self.data[idx]\n",
    "        return (self.transform(data), recognized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n",
      "Balance : 117555 True / 8566 False\n"
     ]
    }
   ],
   "source": [
    "dataset = QuickDrawDataset(\n",
    "    file=\"datasets/sheep.ndjson\",\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "print(f'Balance : {dataset.trueish} True / {dataset.falseish} False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#affichons les premières images\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "    img, label = dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Les images sont sur un format très similaire à ce qu'on a utilisé, on peut suivre la trame utilisée pour MNIST\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.07875107228755951\n",
      "Variance: 0.2692717909812927\n"
     ]
    }
   ],
   "source": [
    "mean, std = 0, 0\n",
    "\n",
    "for image, label in dataloader:\n",
    "    image = image.view(image.size(0), -1)\n",
    "    mean+= image.mean().sum()\n",
    "    std+= image.std().sum()\n",
    "\n",
    "mean /= len(dataloader)\n",
    "std /= len(dataloader)\n",
    "\n",
    "print (f'Mean: {mean}')\n",
    "print (f'Variance: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "# On peut affiner le dataset \n",
    "\n",
    "cleaned_dataset = QuickDrawDataset(\n",
    "    file=\"datasets/sheep.ndjson\",\n",
    "    transform=T.Compose([T.ToTensor(), T.Normalize((mean), (std))])\n",
    ")\n",
    "\n",
    "# Les images sont sur un format très similaire à ce qu'on a utilisé, on peut suivre la trame utilisée pour MNIST\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset)) \n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "training_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(training_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importons de quoi faire une Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "writer = SummaryWriter('runs/sheep_experiment')\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return epoch_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return test_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "# Training loop with added graphing\n",
    "def train_and_test(dataloader_train, dataloader_test, model, loss_fn, optimizer, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss, accuracy = train_loop(dataloader_train, model, loss_fn, optimizer, batch_size)\n",
    "        writer.add_scalars('Loss', {'train': train_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'train': accuracy}, epoch)\n",
    "\n",
    "        # Test the model\n",
    "        test_loss, accuracy = test_loop(dataloader_test, model, loss_fn)\n",
    "        writer.add_scalars('Loss', {'test': test_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'test': accuracy}, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_stack_activation = nn.ReLU\n",
    "        self.mlp_activation = nn.ReLU\n",
    "\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            self.conv_stack_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            self.conv_stack_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 64 * 28 * 28 -> 64 * 14 * 14\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 14 * 14, 128),\n",
    "            self.mlp_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            self.mlp_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2), # mouton ou pas mouton\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(cnn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 1/5 complete\n",
      "\n",
      "Epoch 2/5\n",
      "Epoch 2/5 complete\n",
      "\n",
      "Epoch 3/5\n",
      "Epoch 3/5 complete\n",
      "\n",
      "Epoch 4/5\n",
      "Epoch 4/5 complete\n",
      "\n",
      "Epoch 5/5\n",
      "Epoch 5/5 complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_and_test(train_dataloader, test_dataloader, cnn, loss_fn, optimizer, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs/sheep_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(*, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cpu),\n",
      "      %conv_stack.0.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.0.bias : Float(32, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.3.weight : Float(64, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.3.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.6.weight : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.6.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.6.running_mean : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv_stack.6.running_var : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %mlp.1.weight : Float(128, 12544, strides=[12544, 1], requires_grad=1, device=cpu),\n",
      "      %mlp.1.bias : Float(128, strides=[1], requires_grad=1, device=cpu),\n",
      "      %mlp.4.weight : Float(128, 128, strides=[128, 1], requires_grad=1, device=cpu),\n",
      "      %mlp.4.bias : Float(128, strides=[1], requires_grad=1, device=cpu),\n",
      "      %mlp.7.weight : Float(2, 128, strides=[128, 1], requires_grad=1, device=cpu),\n",
      "      %mlp.7.bias : Float(2, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/conv_stack/conv_stack.0/Conv_output_0 : Float(*, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/conv_stack/conv_stack.0/Conv\"](%input, %conv_stack.0.weight, %conv_stack.0.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.conv.Conv2d::conv_stack.0 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/conv_stack/conv_stack.1/Relu_output_0 : Float(*, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/conv_stack/conv_stack.1/Relu\"](%/conv_stack/conv_stack.0/Conv_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.activation.ReLU::conv_stack.1 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/conv_stack/conv_stack.3/Conv_output_0 : Float(*, 64, 28, 28, strides=[50176, 784, 28, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/conv_stack/conv_stack.3/Conv\"](%/conv_stack/conv_stack.1/Relu_output_0, %conv_stack.3.weight, %conv_stack.3.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.conv.Conv2d::conv_stack.3 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/conv_stack/conv_stack.4/Relu_output_0 : Float(*, 64, 28, 28, strides=[50176, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/conv_stack/conv_stack.4/Relu\"](%/conv_stack/conv_stack.3/Conv_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.activation.ReLU::conv_stack.4 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/conv_stack/conv_stack.6/BatchNormalization_output_0 : Float(*, 64, 28, 28, strides=[50176, 784, 28, 1], requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002, onnx_name=\"/conv_stack/conv_stack.6/BatchNormalization\"](%/conv_stack/conv_stack.4/Relu_output_0, %conv_stack.6.weight, %conv_stack.6.bias, %conv_stack.6.running_mean, %conv_stack.6.running_var), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.batchnorm.BatchNorm2d::conv_stack.6 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2812:0\n",
      "  %/conv_stack/conv_stack.7/MaxPool_output_0 : Float(*, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/conv_stack/conv_stack.7/MaxPool\"](%/conv_stack/conv_stack.6/BatchNormalization_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.pooling.MaxPool2d::conv_stack.7 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/mlp/mlp.0/Flatten_output_0 : Float(*, 12544, strides=[12544, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/mlp/mlp.0/Flatten\"](%/conv_stack/conv_stack.7/MaxPool_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.flatten.Flatten::mlp.0 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/modules/flatten.py:53:0\n",
      "  %/mlp/mlp.1/Gemm_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/mlp.1/Gemm\"](%/mlp/mlp.0/Flatten_output_0, %mlp.1.weight, %mlp.1.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::mlp.1 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/mlp/mlp.2/Relu_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/mlp/mlp.2/Relu\"](%/mlp/mlp.1/Gemm_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::mlp.2 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/mlp/mlp.4/Gemm_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/mlp.4/Gemm\"](%/mlp/mlp.2/Relu_output_0, %mlp.4.weight, %mlp.4.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::mlp.4 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/mlp/mlp.5/Relu_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/mlp/mlp.5/Relu\"](%/mlp/mlp.4/Gemm_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::mlp.5 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %output : Float(*, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/mlp.7/Gemm\"](%/mlp/mlp.5/Relu_output_0, %mlp.7.weight, %mlp.7.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::mlp.7 # /Users/felix/dev_iim/quick-draw-pytorch/src/model/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformation pour avoir un modele exportable\n",
    "\n",
    "cnn.to(\"cpu\")\n",
    "\n",
    "torch_input = torch.randn(1, 1, 28, 28) # N, C, L, T\n",
    "\n",
    "onn_program = torch.onnx.export(\n",
    "    cnn,\n",
    "    torch_input,\n",
    "    \"model.onnx\",\n",
    "    verbose=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    opset_version=11, # /!\\ \n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batchs_size\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
